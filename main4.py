# ======================================================================================
# ì‹¤ì‹œê°„ ì¡°ë¦½ ê³µì • ê²€ì¦ ì‹œìŠ¤í…œ (FastAPI + YOLOv8-OBB) - ë¡œì»¬ ì›¹ìº  ë²„ì „
# ======================================================================================

# --- 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import cv2
import torch
import numpy as np
import asyncio
import base64
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.templating import Jinja2Templates
from ultralytics import YOLO
from contextlib import asynccontextmanager

# --- 2. ì‹œìŠ¤í…œ ì „ì—­ ì„¤ì • (Configuration) ---
MODEL_PATH = "best_last2.pt"  # í•™ìŠµëœ YOLOv8-OBB ëª¨ë¸ íŒŒì¼ì˜ ê²½ë¡œ
CAMERA_INDEX = 0  # ì‚¬ìš©í•  ê¸°ë³¸ ì¹´ë©”ë¼ ì¸ë±ìŠ¤ (0: ë‚´ì¥, 1: ì²« ë²ˆì§¸ ì™¸ì¥ ë“±)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # ì—°ì‚° ì¥ì¹˜ ì„¤ì •
CONF_THRESHOLD = 0.8  # ì‹ ë¢°ë„ ì„ê³„ê°’

# --- 3. ê³µì • ìƒíƒœ ê´€ë¦¬ (State Management) ---
ASSEMBLY_STATE = {
    "current_step": 1,
    "steps_info": {
        1: {"name": "ë¹¨ê°•+ì´ˆë¡", "class": "ì¡°ë¦½1", "message": "1ë‹¨ê³„: ë¹¨ê°• ë¸”ëŸ­ê³¼ ì´ˆë¡ ë¸”ëŸ­ì„ ì¡°ë¦½í•˜ì„¸ìš”."},
        2: {"name": "ë¹¨ê°•+ì´ˆë¡+íŒŒë‘1", "class": "ì¡°ë¦½2", "message": "2ë‹¨ê³„: íŒŒë€ìƒ‰ ì‚¼ê° ë¸”ëŸ­ í•˜ë‚˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”."},
        3: {"name": "ë¹¨ê°•+ì´ˆë¡+íŒŒë‘2", "class": "ì¡°ë¦½3", "message": "3ë‹¨ê³„: ë§ˆì§€ë§‰ íŒŒë€ìƒ‰ ì‚¼ê° ë¸”ëŸ­ì„ ì¶”ê°€í•˜ì„¸ìš”."}
    },
    "completion_message": "ğŸ‰ ì¡°ë¦½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰",
    "error_message": "ì˜ëª»ëœ ì¡°ë¦½ì…ë‹ˆë‹¤. í˜„ì¬ ë‹¨ê³„ì— ë§ê²Œ ë‹¤ì‹œ ì¡°ë¦½í•´ì£¼ì„¸ìš”."
}

# --- 4. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì • ---
templates = Jinja2Templates(directory="template2")

@asynccontextmanager
async def lifespan(app: FastAPI):
    print(f"ì—°ì‚° ì¥ì¹˜: {DEVICE}")
    print("YOLOv8-OBB ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    app.state.model = YOLO(MODEL_PATH)
    
    # ë¡œì»¬ ì›¹ìº  ì—°ê²° ë¡œì§
    print(f"{CAMERA_INDEX}ë²ˆ ì¹´ë©”ë¼ì— ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
    # cv2.CAP_DSHOWëŠ” Windowsì—ì„œ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì†ë„ë¥¼ ë†’ì—¬ì£¼ê³  ì•ˆì •ì„±ì„ ë”í•´ì¤ë‹ˆë‹¤.
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW) 
    
    if not cap.isOpened():
        print(f"{CAMERA_INDEX}ë²ˆ ì¹´ë©”ë¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 0ë²ˆ ì¹´ë©”ë¼ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")
        cap.release()
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    
    if not cap.isOpened():
        print("ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        app.state.cap = None
    else:
        print("ì¹´ë©”ë¼ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        app.state.cap = cap
    
    yield
    
    print("ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    if app.state.cap:
        app.state.cap.release()
        print("ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ê°€ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

app = FastAPI(lifespan=lifespan)

# --- 5. ë³´ì¡° í•¨ìˆ˜ ì •ì˜ ---
def center_crop_square(frame: np.ndarray) -> np.ndarray:
    h, w, _ = frame.shape
    min_dim = min(h, w)
    start_x, start_y = (w - min_dim) // 2, (h - min_dim) // 2
    return frame[start_y:start_y + min_dim, start_x:start_x + min_dim]

def update_assembly_state(detected_classes: list):
    current_step = ASSEMBLY_STATE["current_step"]
    
    if current_step > 3:
        return ASSEMBLY_STATE["completion_message"]

    expected_class = ASSEMBLY_STATE["steps_info"][current_step]["class"]
    
    if expected_class in detected_classes:
        if current_step < 3:
            ASSEMBLY_STATE["current_step"] += 1
            next_step_info = ASSEMBLY_STATE["steps_info"][current_step + 1]
            return f"ì„±ê³µ! ë‹¤ìŒ ë‹¨ê³„: {next_step_info['message']}"
        else:
            ASSEMBLY_STATE["current_step"] += 1
            return ASSEMBLY_STATE["completion_message"]
    elif detected_classes:
        return ASSEMBLY_STATE["error_message"]
    else:
        return ASSEMBLY_STATE["steps_info"][current_step]["message"]

# --- 6. FastAPI ì—”ë“œí¬ì¸íŠ¸(API ê²½ë¡œ) ì •ì˜ ---
@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse("index4.html", {"request": request})

@app.post("/reset")
async def reset_state():
    ASSEMBLY_STATE["current_step"] = 1
    print("ê³µì • ìƒíƒœê°€ 1ë‹¨ê³„ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return {"message": "State reset successfully"}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    cap = websocket.app.state.cap
    model = websocket.app.state.model

    if not cap:
        await websocket.close(code=1011, reason="Camera not available")
        return

    try:
        frame_counter = 0
        while True:
            success, frame = cap.read()
            if not success:
                print("í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨. ì¹´ë©”ë¼ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
                break
            
            frame_counter += 1

            # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¶”ë¡ 
            cropped = center_crop_square(frame)
            resized = cv2.resize(cropped, (640, 640))
            results = model(resized, device=DEVICE, verbose=False, conf=CONF_THRESHOLD)

            # í´ë˜ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            detected_classes = []
            result = results[0]
            
            if result.obb and len(result.obb) > 0:
                if frame_counter % 30 == 0:
                    print(f"[Frame {frame_counter}] OBBs found: {len(result.obb)}")
                
                class_indices_tensor = result.obb.cls
                if class_indices_tensor is not None:
                    class_indices = class_indices_tensor.cpu().numpy().astype(int).tolist()
                    
                    if frame_counter % 30 == 0:
                        print(f"  - ê°ì§€ëœ í´ë˜ìŠ¤ ì¸ë±ìŠ¤: {class_indices}")
                        print(f"  - ëª¨ë¸ì˜ ì „ì²´ í´ë˜ìŠ¤ ì´ë¦„: {result.names}")
                    
                    try:
                        detected_classes = sorted(list(set(result.names[i] for i in class_indices)))
                        if detected_classes and frame_counter % 30 == 0:
                            print(f"  - [ì„±ê³µ] ìµœì¢… ê°ì§€ëœ í´ë˜ìŠ¤ ì´ë¦„: {detected_classes}")
                    except Exception as e:
                        print(f"  - [ì˜¤ë¥˜] í´ë˜ìŠ¤ ì´ë¦„ ë³€í™˜ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ë°ì´í„° ì „ì†¡
            status_message = update_assembly_state(detected_classes)
            annotated_frame = result.plot()
            _, buffer = cv2.imencode('.jpg', annotated_frame)
            jpg_as_text = base64.b64encode(buffer).decode('utf-8')

            await websocket.send_json({
                "image": jpg_as_text,
                "current_step": ASSEMBLY_STATE["current_step"],
                "message": status_message,
                "detected_classes": detected_classes
            })
            await asyncio.sleep(0.03)

    except WebSocketDisconnect:
        print("í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await websocket.close()